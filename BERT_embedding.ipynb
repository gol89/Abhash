{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tadN4hSCP9p"
   },
   "source": [
    "# Twitter Sentiment Analysis using BERT embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EUU4TlmoFMZ_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fOfuPdFHFpfC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9S77lewDNE1"
   },
   "source": [
    "### Loading the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "slnILsqwGxTX"
   },
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "data = pd.read_csv(\n",
    "    \"training.1600000.processed.noemoticon.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "REdK4z4YG9kZ"
   },
   "outputs": [],
   "source": [
    "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz2g61evDZb4"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCyy4babDrI8"
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UEyorQS_HArn"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    # Removing the @\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    # Removing the URL links\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    # Keeping only letters\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    # Removing additional whitespaces\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking only 10 percent of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_short = data.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160000, 2), 320016)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_short.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of postive sentiments :-  0.500025\n"
     ]
    }
   ],
   "source": [
    "print('The fraction of postive sentiments :- ',len(data_short[data_short['sentiment']==4])/len(data_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus the shortened Dataset is a balanced one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3BlbZpy0HHiV"
   },
   "outputs": [],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data_short.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1514154</th>\n",
       "      <td>4</td>\n",
       "      <td>@Epicx they are pretty astro. think I may have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730651</th>\n",
       "      <td>0</td>\n",
       "      <td>work. sunday. work. *sigh*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "1514154          4  @Epicx they are pretty astro. think I may have...\n",
       "730651           0                        work. sunday. work. *sigh* "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_short[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' they are pretty astro. think I may have to download their music mate '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "H6SOj46BHKEk"
   },
   "outputs": [],
   "source": [
    "data_labels = data_short.sentiment.values\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000 (160000,)\n"
     ]
    }
   ],
   "source": [
    "print(len(data_clean) , data_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJa3YWeJD1gM"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUaCPqqBD7kQ"
   },
   "source": [
    "We need to create a BERT layer to have access to meta data for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0wry-st-HMN0"
   },
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMVarTJpELyK"
   },
   "source": [
    "We only use the first sentence for BERT inputs so we add the CLS token at the beginning and the SEP token at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "v-JkZt9NduoC"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "pel_Uk6Ic4xB"
   },
   "outputs": [],
   "source": [
    "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'well',\n",
       " 'he',\n",
       " 'needs',\n",
       " 'the',\n",
       " 'alarm',\n",
       " 'clock',\n",
       " 'reset',\n",
       " 'then',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z32MeEwnkCB8"
   },
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUVc83VNEcW9"
   },
   "source": [
    "We need to create the 3 different inputs for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "wmW9JZLJaxww"
   },
   "outputs": [],
   "source": [
    "def get_ids(tokens):\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def get_mask(tokens):\n",
    "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
    "\n",
    "def get_segments(tokens):\n",
    "    seg_ids = []\n",
    "    current_seg_id = 0\n",
    "    for tok in tokens:\n",
    "        seg_ids.append(current_seg_id)\n",
    "        if tok == \"[SEP]\":\n",
    "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
    "    return seg_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x06fFPFtFqVK"
   },
   "source": [
    "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "hjAVGCwlb6F8"
   },
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i], len(sent)]\n",
    "                 for i, sent in enumerate(data_inputs)]\n",
    "random.shuffle(data_with_len)\n",
    "data_with_len.sort(key=lambda x: x[2])\n",
    "sorted_all = [([get_ids(sent_lab[0]),\n",
    "                get_mask(sent_lab[0]),\n",
    "                get_segments(sent_lab[0])],\n",
    "               sent_lab[1])\n",
    "              for sent_lab in data_with_len if sent_lab[2] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  'cc',\n",
       "  '##tv',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'paperwork',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'cc',\n",
       "  '##tv',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'lunch',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'cc',\n",
       "  '##tv',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'sack',\n",
       "  'nah',\n",
       "  '##eed',\n",
       "  '##ul',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'cc',\n",
       "  '##tv',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'home',\n",
       "  '[SEP]'],\n",
       " 0,\n",
       " 102]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_len[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144484"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[101,\n",
       "   10507,\n",
       "   9189,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   17397,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   10507,\n",
       "   9189,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   6265,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   10507,\n",
       "   9189,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   12803,\n",
       "   20976,\n",
       "   13089,\n",
       "   5313,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   10507,\n",
       "   9189,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   2188,\n",
       "   102],\n",
       "  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_all[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "ZkMiqmzsfo6a"
   },
   "outputs": [],
   "source": [
    "# A list is a type of iterator so it can be used as generator for a dataset\n",
    "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "gkGWlzeOfos6"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE,\n",
    "                                       padded_shapes=((3, None), ()),\n",
    "                                       padding_values=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "5aA7it--hHl4"
   },
   "outputs": [],
   "source": [
    "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
    "NB_BATCHES_TEST = NB_BATCHES // 10\n",
    "all_batched.shuffle(NB_BATCHES)\n",
    "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4516"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_BATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_BATCHES_TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "X4QCPok7aEM_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 3, 10), dtype=int32, numpy=\n",
       " array([[[  101,  2053,  3291,  2428,  3246,  2017,  2424,  2115,  4937,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  1045,  4299,  1045,  2001,  2004,  4427,  2004,  2017,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  4372,  7913, 11365,  2696,  4012, 14833,  2022,  7474,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  5983,  2026,  8808, 17955, 10261,  2001,  2182,  1012,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  2087,  3819,  4633,  1999,  1996,  2878,  2898,  2088,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  7367,  6299,  2075,  7208,  2507,  2033, 26836,  1012,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  2003,  3564,  2006,  1996, 11673,  3666,  1996,  4153,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  1045,  2128,  1038, 15000,  5669,  2009,  2005,  2017,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  2008,  2052,  2022,  1037,  3959,  6050,  5262,  1012,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  2045,  2442,  2022,  1037,  8579,  2041,  2045,  4873,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  3403,  5443,  2000,  4638,  1999,  2000,  1056, 10343,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101, 17012,  2941,  2204,  2154,  2012,  2147,  2061,  2521,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  4840,  2358, 10376, 20968,  3622,  2013,  1996,  3871,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  4485,  2008, 10514,  9468,  9468,  9468, 10603,   999,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101, 22708,  2082,  4826,  2031,  1037,  3647,  3462,  4826,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  4299,  1045,  2071,  1005,  2310,  2908,  1012,  1012,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  6603,  2339,  3475,  1005,  1056, 10474,  2075,  3892,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  7592,  9541,   999,  2129,  1054,  1057,  2651,  1029,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  2067,  2188,  2073,  1996,  2293,  2003,  2012,  1012,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  3566,  2073,  2412,  2017,  2024,  3407, 10756,  2154,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  3850, 15908,  2053, 12090, 11287,  9061,  9061,  3850,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  3554,  1996, 15450,  2645,  1012,  1012,  1012,  1012,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  4931, 18138,  9072,  2004,  1037,  2521,  2102,  3835,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  2994,  2290,  3500,  7822,  1005,  2022,  2067,  4465,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  1012,  1012,  1045,  2054,  1996,  9457,  5888,   999,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  3067,  2205,  1012,  2074,  2025,  2026,  7473,  1012,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  2009,  2428, 19237,  2008,  2027,  2024,  5306,  2290,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  3426,  2111,  2024, 17372,  2041,   999,  2004,  4140,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101, 13869,  1012,  1012,  1012,  1012,  1012,  2393,   999,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  3337,  2058,  4870,  2003,  2746,  2000,  1996,  5137,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  7929, 11057, 11057,  2100,  4067, 19658, 19658,  2015,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]],\n",
       " \n",
       "        [[  101,  2374,  1012,  1012,  1012,  2025,  2205,  2204,  2295,\n",
       "            102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]]])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 1, 0])>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2pxAPFxGe8r"
   },
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "L6DD3k3qPLDQ"
   },
   "outputs": [],
   "source": [
    "class DCNNBERTEmbedding(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNNBERTEmbedding, self).__init__(name=name)\n",
    "        \n",
    "        self.bert_layer = hub.KerasLayer(\n",
    "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "            trainable=False)\n",
    "\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=4,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def embed_with_bert(self, all_tokens):\n",
    "        _, embs = self.bert_layer([all_tokens[:, 0, :],\n",
    "                                   all_tokens[:, 1, :],\n",
    "                                   all_tokens[:, 2, :]])\n",
    "        return embs\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.embed_with_bert(inputs)\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsWpzQz2IQvJ"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "lhfUFvWEPOIf"
   },
   "outputs": [],
   "source": [
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "5HPbZ72KPPnX"
   },
   "outputs": [],
   "source": [
    "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
    "                         FFN_units=FFN_UNITS,\n",
    "                         nb_classes=NB_CLASSES,\n",
    "                         dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "JpHDseF0QLl3"
   },
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow_hub.keras_layer.KerasLayer at 0x21f06476160>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x21f109bf310>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x21f109bf1c0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x21f109bb730>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D at 0x21f109bf490>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x21f109bbdc0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x21f109b69d0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x21f109b6b20>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "K1hdT_JT2Rfi",
    "outputId": "1dea9a59-b1dd-43e4-b50a-00122225ee28"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"ckpt_bert_embedding/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "e8LHztku2cjl"
   },
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0C5lNxFTMrA"
   },
   "source": [
    "## Training the model on the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "WrT8oWZzQNmW",
    "outputId": "3f196a33-cbe1-4e71-e2ef-59fe1ea0d4dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4065/Unknown - 7940s 2s/step - loss: 0.4187 - accuracy: 0.8087Checkpoint saved at ckpt_bert_embedding/.\n",
      "4065/4065 [==============================] - 7943s 2s/step - loss: 0.4187 - accuracy: 0.8087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21f11211160>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn.fit(train_dataset,\n",
    "         epochs=1,\n",
    "         callbacks=[MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAb_ijA5Idmz"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "gQN-Y99WIf6m",
    "outputId": "8ff576c9-59ea-49fb-f186-279fa8c8e299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 768)\n",
      "451/451 [==============================] - 205s 455ms/step - loss: 0.3808 - accuracy: 0.8405\n",
      "[0.3807750642299652, 0.8404933214187622]\n"
     ]
    }
   ],
   "source": [
    "results = Dcnn.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "Rj98dgxnmhak"
   },
   "outputs": [],
   "source": [
    "def get_prediction(sentence):\n",
    "    tokens = encode_sentence(sentence)\n",
    "\n",
    "    input_ids = get_ids(tokens)\n",
    "    input_mask = get_mask(tokens)\n",
    "    segment_ids = get_segments(tokens)\n",
    "\n",
    "    inputs = tf.stack(\n",
    "        [tf.cast(input_ids, dtype=tf.int32),\n",
    "         tf.cast(input_mask, dtype=tf.int32),\n",
    "         tf.cast(segment_ids, dtype=tf.int32)],\n",
    "         axis=0)\n",
    "    inputs = tf.expand_dims(inputs, 0) # simulates a batch\n",
    "\n",
    "    output = Dcnn(inputs, training=False)\n",
    "\n",
    "    sentiment = math.floor(output*2)\n",
    "\n",
    "    if sentiment == 0:\n",
    "        print(\"Output of the model: {}\\nPredicted sentiment: negative\".format(\n",
    "            output))\n",
    "    elif sentiment == 1:\n",
    "        print(\"Output of the model: {}\\nPredicted sentiment: positive\".format(\n",
    "            output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "J9jC8UnJgOjS",
    "outputId": "808b061b-e80e-4f3c-ecda-3dba81606e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 768)\n",
      "Output of the model: [[0.93041694]]\n",
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"This actor is a awesome.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn.save_weights('Dcnn_Bert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12, 768)\n",
      "Output of the model: [[0.2719274]]\n",
      "Predicted sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Awww!!!! Mumbai is Humid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 768)\n",
      "Output of the model: [[0.84825623]]\n",
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"wow !!!! Mumbai is Humid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 768)\n",
      "Output of the model: [[0.68753064]]\n",
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"I passsssssssssssed \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000021EF6419EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000021EF6419EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9, 768)\n",
      "Output of the model: [[0.9707856]]\n",
      "Predicted sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Today the weather is sunny and bright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn.save_weights('Dcnn_Bert3.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn.save_weights('Dcnn_Bert4.tf',save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 768)\n",
      "Output of the model: [[0.00794217]]\n",
      "Predicted sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"i lost my car today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 768)\n",
      "Output of the model: [[0.07735083]]\n",
      "Predicted sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"The economy is going down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 768)\n",
      "Output of the model: [[0.24358734]]\n",
      "Predicted sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Situation on the border escalates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our model is able to correctly predict the sentiments conveyed in a sentence."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_embedding",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
